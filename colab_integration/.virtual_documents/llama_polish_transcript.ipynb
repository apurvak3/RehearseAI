!pip install transformers
!pip install huggingface_hub
!pip install bitsandbytes
!pip install accelerate


import sys
sys.path.append('path_to_secret_folder')

from secret import userdata



pip install huggingface_hub



from huggingface_hub import login
hf_token=userdata.get('HF_TOKEN')
login(hf_token, add_to_git_credential=True)


LLAMA_MODEL = "meta-llama/Meta-Llama-3-8B-Instruct"


import os
import requests
from IPython.display import Markdown, display, update_display
from openai import OpenAI

from huggingface_hub import login
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig
import torch


import os
from dotenv import load_dotenv

# Load the environment variables from .env file
load_dotenv()

# Get the token from environment variables
HF_TOKEN = os.getenv("HF_TOKEN")

print(HF_TOKEN)  # Check if the token is loaded properly



from transformers import AutoTokenizer, AutoModelForCausalLM

OLLAMA_API = "http://localhost:11434/api/chat"
MODEL = "llama3"
tokenizer = AutoTokenizer.from_pretrained(LLAMA_MODEL, use_auth_token=HF_TOKEN)
model = AutoModelForCausalLM.from_pretrained(LLAMA_MODEL, device_map="auto", quantization_config=quant_config, use_auth_token=HF_TOKEN)

# Test output
print("Model and tokenizer loaded successfully!")




